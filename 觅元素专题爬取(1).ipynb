{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读取文件\n",
      "读取文件成功\n",
      "输出proxiesHTTP://182.35.86.121:9999\n",
      "更换代理中...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-775af8e1edb4>\u001b[0m in \u001b[0;36mget_html\u001b[1;34m(utl, http_list, https_list)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"http\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"https\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttps_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"User-Agent\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mUser_Agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"cookie\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrevert_cookie\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcookie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'revert_cookie' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-775af8e1edb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[0mfirst_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'http://www.51yuansu.com/subject//'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m     \u001b[0mfirst_html\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhttp_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhttps_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m     \u001b[0mfirst_url_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfirst_title\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_first_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_html\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[0mget_photolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_url_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfirst_title\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhttp_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhttps_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-775af8e1edb4>\u001b[0m in \u001b[0;36mget_html\u001b[1;34m(utl, http_list, https_list)\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"http\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"https\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttps_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"User-Agent\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUser_Agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "import requests,time\n",
    "import os,sys,cv2,random\n",
    "from lxml import etree\n",
    "from PIL import Image\n",
    "from selenium import webdriver\n",
    "#设置多个user—agent\n",
    "User_Agent = random.choice([\n",
    "        \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "        \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "        \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "        \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "        \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "        \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n",
    "        \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n",
    "        \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36',\n",
    "    ])\n",
    "def req(url):\n",
    "    #获取动态连接池\n",
    "    # 设置多个 user-agent\n",
    "    # 从http://www.xicidaili.com/爬取代理，并存储到http_list.txt和https_list.txt的文件中\n",
    "    l=os.listdir(os.getcwd())\n",
    "    if \"http_list.txt\" not in l:\n",
    "        proxy_url='http://www.xicidaili.com/'\n",
    "        driver=webdriver.Chrome()\n",
    "        driver.get(proxy_url)\n",
    "        t=driver.page_source\n",
    "        driver.quit()\n",
    " \n",
    "        html=etree.HTML(t)\n",
    "        http_list=[]\n",
    "        https_list=[]\n",
    "        for i in (list(range(3,23)) + list(range(25,45))):\n",
    "            ip=html.xpath('//*[@id=\"ip_list\"]/tbody/tr[{}]/td[2]/text()'.format(i))[0].strip()\n",
    "            port=html.xpath('//*[@id=\"ip_list\"]/tbody/tr[{}]/td[3]/text()'.format(i))[0].strip()\n",
    "            protocol=html.xpath('//*[@id=\"ip_list\"]/tbody/tr[{}]/td[6]/text()'.format(i))[0].strip()\n",
    "            if protocol==\"HTTP\":\n",
    "                http_list.append(protocol+r\"://\"+ip+\":\"+port)\n",
    "            if protocol == \"HTTPS\":\n",
    "                https_list.append(protocol + r\"://\" + ip + \":\" + port)\n",
    "        with open(\"http_list.txt\",\"w\") as p:\n",
    "            p.write(str(http_list))\n",
    "        with open(\"https_list.txt\", \"w\") as p:\n",
    "            p.write(str(https_list))\n",
    "        print('生成文件成功')\n",
    "    else:\n",
    "        print('开始读取文件')\n",
    "        with open(\"http_list.txt\") as f:\n",
    "            http_list = eval(f.read())\n",
    "        with open(\"https_list.txt\") as f:\n",
    "            https_list = eval(f.read())\n",
    "        print('读取文件成功')\n",
    "        print('输出proxies'+http_list[0])\n",
    "    return http_list,https_list\n",
    "\n",
    "def get_html(utl,http_list,https_list):\n",
    "    try:\n",
    "        proxies={\"http\":random.choice(http_list),\"https\":random.choice(https_list)}\n",
    "        headers={\"User-Agent\":User_Agent,\"cookie\":revert_cookie(cookie)}\n",
    "        r=requests.get(url,proxies=proxies,headers=headers)\n",
    "        print(r.status_code)\n",
    "        return r.text\n",
    "    except:\n",
    "        print(\"更换代理中...\")\n",
    "        while True:\n",
    "            for n in range(10):\n",
    "                proxies = {\"http\": random.choice(list(http_list)), \"https\": random.choice(list(https_list))}\n",
    "                headers = {\"User-Agent\": User_Agent,}\n",
    "                r = requests.get(url, proxies=proxies, headers=headers)\n",
    "                if r.status_code==200:\n",
    "                    print(r.status_code)\n",
    "                    return r.text\n",
    "                else:\n",
    "                    continue\n",
    "            print('无法连接服务器...')\n",
    "\n",
    "def get_first_url(data):\n",
    "    '''提取最初页的内容'''\n",
    "    html=etree.HTML(data)\n",
    "    try:\n",
    "        url_list=html.xpath('//a[@class=\"sj-ys-box\"]/@href')\n",
    "        page_title=html.xpath('//p[@class=\"subject-ys-p\"]/text()')\n",
    "    #现在获取到标题名称，那么我们要根据标题名称去建立文件夹\n",
    "        for title in page_title:\n",
    "            path='F:\\觅元素原图\\免扣元素专题'+'\\\\'+title\n",
    "            read_dir(path)\n",
    "    except:\n",
    "        print('提取内容出错')\n",
    "    return url_list,page_title\n",
    "\n",
    "\n",
    "def get_change_page(data):\n",
    "    '''换页'''\n",
    "    time.sleep(5)\n",
    "    html=etree.HTML(data)\n",
    "    try:\n",
    "        url=html.xpath('//a[@class=\"nextpage\"]/@href')\n",
    "    except:\n",
    "        print('换页失败')\n",
    "    return url[0]\n",
    "\n",
    "def get_page_url(data):\n",
    "    '''提取缩略页'''\n",
    "    html=etree.HTML(data)\n",
    "    try:\n",
    "        url_list=html.xpath('//a[@class=\"img-wrap\"]/@href')\n",
    "    except:\n",
    "        print('提取列表页出错')\n",
    "    return url_list\n",
    "\n",
    "def get_img_url(data,url):\n",
    "    '''提取高清大图URL'''\n",
    "    html=etree.HTML(data)\n",
    "    try:\n",
    "        url=html.xpath('//img[@class=\"show-image\"]/@src')[0]\n",
    "        title=html.xpath('//img[@class=\"show-image\"]/@alt')[0]+'.jpg'\n",
    "    except:\n",
    "        print('出现异常图片')\n",
    "        url='http://pic.51yuansu.com//pic3//cover//03//35//71//5b8dc0a6366bd_610.jpg'\n",
    "        title='替换异常图片'+'.jpg'\n",
    "        time.sleep(10)\n",
    "    return url,title\n",
    "\n",
    "def get_img(url,file,path,http_list,https_list):\n",
    "    '''下载图片'''\n",
    "    file_name=path+'\\\\'+file\n",
    "    try:\n",
    "        proxies={\"http\":random.choice(http_list),\"https\":random.choice(https_list)}\n",
    "        headers={\"User-Agent\":User_Agent,\"cookie\":revert_cookie(cookie)}\n",
    "        img=requests.get(url,proxies=proxies,headers=headers).content\n",
    "        with open(file_name,'wb') as save_img:\n",
    "            save_img.write(img)\n",
    "    except:\n",
    "        print('下载过程出错，这张图片跳过')\n",
    "        \n",
    "def re_backcolor(path,filename,new_path):\n",
    "    '''图片去除背景色'''\n",
    "    img_path=path+'\\\\'+filename\n",
    "    print(img_path)\n",
    "    str_list=filename.split('.')\n",
    "    new_path=new_path+'\\\\'+str_list[0]+'.png'\n",
    "    print(new_path)\n",
    "    img = Image.open(img_path)\n",
    "    img = img.convert(\"RGBA\")  # 转换获取信息\n",
    "    pixdata = img.load()\n",
    "    for y in range(img.size[1]):\n",
    "        for x in range(img.size[0]):\n",
    "            if pixdata[x, y][0] > 220 and pixdata[x, y][1] > 220 and pixdata[x, y][2] > 220 and pixdata[x, y][3] > 220:\n",
    "                pixdata[x, y] = (255, 255, 255, 0)\n",
    "    img.save(new_path)\n",
    "    \n",
    "def read_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "def get_photolist(url_list,first_title,http_list,https_list):\n",
    "    count=1\n",
    "    secondcount=0\n",
    "    for j in first_url_list:\n",
    "        print(j)\n",
    "        flag=1000\n",
    "        second_html=get_html(j,http_list,https_list)\n",
    "        next_page=get_change_page(second_html)\n",
    "        second_url_list=get_page_url(second_html)\n",
    "        dir_name='F:\\觅元素原图\\免扣元素专题'+'\\\\'+first_title[secondcount]\n",
    "        secondcount=secondcount+1\n",
    "        while  not next_page is None:\n",
    "            for k in second_url_list:\n",
    "                if count%100==0:\n",
    "                    time.sleep(5)\n",
    "                third_html=get_html(k,http_list,https_list)\n",
    "                third_url,third_title=get_img_url(third_html,k)\n",
    "                get_img(third_url,third_title,dir_name,http_list,https_list)\n",
    "                print(count)\n",
    "                print('正在下载{}'.format(third_title))\n",
    "                count=count+1\n",
    "            headers={'User-Agent':'Mozilla/5.0(Windows NT 10.0; WOW64;rv:66.0)Gecko/20100101 Firefox/66.0',\n",
    "                    'Referer':second_html,}\n",
    "            second_html=get_html(next_page,http_list,https_list)\n",
    "            current_page=next_page\n",
    "            print('当前页'+current_page)\n",
    "            next_page=get_change_page(second_html)\n",
    "            print('下一页'+next_page)\n",
    "            second_url_list=get_page_url(second_html)\n",
    "            if current_page==next_page:\n",
    "                break\n",
    "            \n",
    "        for k in second_url_list:\n",
    "                print('最后一页')\n",
    "                if count%100==0:\n",
    "                    time.sleep(5)\n",
    "                third_html=get_html(k,http_list,https_list)\n",
    "                third_url,third_title=get_img_url(third_html,k)\n",
    "                get_img(third_url,third_title,dir_name,http_list,https_list)\n",
    "                print(count)\n",
    "                print('正在下载{}'.format(third_title))\n",
    "                count=count+1\n",
    "\n",
    "http_list,https_list=req('http://www.51yuansu.com/subject/118-1.html')\n",
    "for i in range(4):\n",
    "    '''专题页一共四页'''\n",
    "    i=i+1\n",
    "    first_url='http://www.51yuansu.com/subject//'+str(i)+'/'\n",
    "    first_html=get_html(first_url,http_list,https_list)\n",
    "    first_url_list,first_title=get_first_url(first_html)\n",
    "    get_photolist(first_url_list,first_title,http_list,https_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
